{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Social Diary\n",
    "\n",
    "[New York Social Diary](http://www.newyorksocialdiary.com/) casts a fascinating lens onto New York's socially well-to-do.  The data forms a natural social graph for New York's social elite.  Take a look at this page of a recent run-of-the-mill holiday party:\n",
    "\n",
    "`http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers`\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together. In this project, we will scrape data from this website, parse the captions to find which people occur in photos together, and build a social graph of the result.\n",
    "\n",
    "The first step is to fetch the data.  This comes in two phases.\n",
    "\n",
    "The first step is to crawl the data.  We want photos from parties before December 1st, 2014.  Go to\n",
    "`http://www.newyorksocialdiary.com/party-pictures`\n",
    "to see a list of (party) pages.  For each party's page, grab all the captions.\n",
    "\n",
    "*Hints*:\n",
    "\n",
    "  1. Click on the on the index page and see how they change the url.  Use this to determine a strategy to get all the data.\n",
    "\n",
    "  2. Notice that each party has a date on the index page. You can use python's `datetime.strptime` function to parse it.\n",
    "\n",
    "  3. Some captions are not useful: they contain long narrative texts that explain the event.  Usually in two stage processes like this, it is better to keep more data in the first stage and then filter it out in the second stage.  This makes your work more reproducible.  It's usually faster to download more data than you need now than to have to redownload more data later.\n",
    "\n",
    "Now that you have a list of all captions, you should probably save the data on disk so that you can quickly retrieve it.  Now comes the parsing part.\n",
    "\n",
    "  1. Try to find some heuristic rules to separate captions that are a list of names from those that are not. For one, consider that long captions are often not lists of people.  The cutoff is subjective so to be definitive, *let's set that cutoff at 250 characters*.\n",
    "\n",
    "  2. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`.\n",
    "\n",
    "  3. You might find a person named \"ra Lebenthal\".  There is no one by this name.  Can anyone spot what's happening here?\n",
    "\n",
    "  4. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other ('optional') titles that are being used?  They should probably be filtered out b/c they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "\n",
    "For the analysis, we think of the problem in terms of a [network](http://en.wikipedia.org/wiki/Computer_network) or a [graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29).  Any time a pair of people appear in a photo together, that is considered a link.  What we have described is more appropriately called an (undirected) [multigraph](http://en.wikipedia.org/wiki/Multigraph) with no self-loops but this has an obvious analog in terms of an undirected [weighted graph](http://en.wikipedia.org/wiki/Graph_%28mathematics%29#Weighted_graph).  In this problem, we will analyze the social graph of the new york social elite.\n",
    "\n",
    "For this problem, we recommend using python's `networkx` library.\n",
    "\n",
    "\n",
    "## I. Degree\n",
    "\n",
    "The simplest question you might want to ask is 'who is the most popular'?  The easiest way to answer this question is to look at how many connections everyone has.  Write a function that returns the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "  \n",
    "*Checkpoint:*\n",
    "\n",
    "    Top 100 .describe()\n",
    "    count    100.000000\n",
    "    mean     106.340000\n",
    "    std       51.509579\n",
    "    min       69.000000\n",
    "    25%       77.000000\n",
    "    50%       85.500000\n",
    "    75%      116.500000\n",
    "    max      372.000000\n",
    "\n",
    "\n",
    "## II. PageRank\n",
    "\n",
    " A similar way to determine popularity is to look at their [pagerank](http://en.wikipedia.org/wiki/PageRank).  Pagerank is used for web ranking and was originally [patented](http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=6285999) by Google and is essentially the [stationary distribution](http://en.wikipedia.org/wiki/Markov_chain#Stationary_distribution_relation_to_eigenvectors_and_simplices) of a [markov chain](http://en.wikipedia.org/wiki/Markov_chain) implied by the social graph.\n",
    "\n",
    "Use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "*Checkpoint:*\n",
    "\n",
    "    Topp 100 .describe()\n",
    "    count    100.000000\n",
    "    mean       0.000185\n",
    "    std        0.000076\n",
    "    min        0.000124\n",
    "    25%        0.000138\n",
    "    50%        0.000162\n",
    "    75%        0.000200\n",
    "    max        0.000623\n",
    "   \n",
    "\n",
    "## III. Best Friends\n",
    "\n",
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights. Write a function which returns a list of 100 tuples of the form ((person1, person2), count) in descending order of count\n",
    "\n",
    "    Topp 100 .describe()\n",
    "    count    100.000000\n",
    "    mean      25.070000\n",
    "    std       15.647154\n",
    "    min       13.000000\n",
    "    25%       15.000000\n",
    "    50%       19.000000\n",
    "    75%       28.500000\n",
    "    max      107.000000\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "from collections import namedtuple\n",
    "import re\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "from urllib import request\n",
    "import csv\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ['TZ'] = 'USA/New York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #download the html pages which contain links to the picture pages. from page 3 to page 29 (as all contain dates within range)\n",
    " #the first page is specified by  http://www.newyorksocialdiary.com/party-pictures?page=3 \n",
    "def loadpage(x):\n",
    "    request.urlretrieve('http://www.newyorksocialdiary.com/party-pictures?page={}'.format(x), 'party-pictures/page_{}.html'.format(x))\n",
    "\n",
    "p = Pool(10) # the max number of webpages to get at once\n",
    "p.map(loadpage, range(3,29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.newyorksocialdiary.com/party-pictures/2015/treasures-of-new-york\n"
     ]
    },
    {
     "ename": "MaybeEncodingError",
     "evalue": "Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x7f70684d8358>'. Reason: 'TypeError(\"cannot serialize '_io.BufferedReader' object\",)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mMaybeEncodingError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5f55e69c216b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the max number of webpages to get at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#print(au)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vagrant/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vagrant/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaybeEncodingError\u001b[0m: Error sending result: '<multiprocessing.pool.ExceptionWithTraceback object at 0x7f70684d8358>'. Reason: 'TypeError(\"cannot serialize '_io.BufferedReader' object\",)'"
     ]
    }
   ],
   "source": [
    "def get_page_urls(): \n",
    "    \n",
    "    # get the filenames of all party-pictures pages which contain links the pages that contain the pcitures with captians\n",
    "    page_urls = []\n",
    "    for x in range(3,29):\n",
    "        page_urls.append(os.path.dirname(os.path.realpath('NewYorkSocialDiary.ipynb'))+'/party-pictures/page_{}.html'.format(x))\n",
    "    return page_urls\n",
    "\n",
    "def get_album_urls(page_urls):\n",
    "    \n",
    "    # scrap and download all the html pages of the party pictures. The first page that contains the links specified by\n",
    "    #url_stub concat '?page=3' as the dates on the page are greater and also less than december 1 2015.  \n",
    "    #subsequent pages have `?page=X` appended, where X is an\n",
    "    #integer from 4 to 28.\n",
    "    album_urls = []\n",
    "    \n",
    "    for url in page_urls:\n",
    "        # load the page\n",
    "        f = open(url, 'r')\n",
    "        s = f.read()\n",
    "        # get the text for parsing\n",
    "        soup = BeautifulSoup(s,\"lxml\")\n",
    "\n",
    "        # select the chunk of html corresponding to each album listing\n",
    "        album_divs = soup.select('div.view-content > div')\n",
    "\n",
    "        #  get the url and year from this html chunk \n",
    "        for album in album_divs:\n",
    "            # get year and date html\n",
    "            l_date = album.select('span.views-field-created > span')\n",
    "            l_url = album.select('span.views-field-title > span > a')\n",
    "        \n",
    "            # extract date\n",
    "            date = datetime.strptime(l_date[0].text, '%A, %B %d, %Y')\n",
    "            # ref date which to return the urls whose dates are before\n",
    "            refdate = datetime(2015, 12, 1)\n",
    "            url = l_url[0]['href']\n",
    "            if date < refdate:\n",
    "                album_urls.append('http://www.newyorksocialdiary.com' + url)\n",
    "            \n",
    "    return album_urls\n",
    "\n",
    "au = get_album_urls(get_page_urls()) # gets all the required urls which contian the captians\n",
    "print(au[0])\n",
    "\n",
    "def loadpage(x):\n",
    "    # downloads the page to disk for faster retrieval later\n",
    "    a = x.rfind('/')\n",
    "    request.urlretrieve(x, 'party-pictures/Pictures/{}.html'.format(x[a+1:]+'-'+x[x[:a].rfind('/')+1:a]))\n",
    "\n",
    "p = Pool(10) # the max number of webpages to get at once\n",
    "p.map(loadpage, au) # use multiprocessing to download multiple pages in parallel\n",
    "\n",
    "#print(au)\n",
    "print(len(au))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "print(len(au))\n",
    "def get_captains_from_album_urls(album_urls):\n",
    "    # return the captains underneath all the pictures form all the urls\n",
    "    captains = []\n",
    "    for url in album_urls:\n",
    "        a = url.rfind('/')\n",
    "        # load the page\n",
    "        f = open(os.path.dirname(os.path.realpath('NewYorkSocialDiary.ipynb'))+\n",
    "                 '/party-pictures/Pictures/{}.html'.format(url[a+1:]+'-'+url[url[:a].rfind('/')+1:a]), 'r')\n",
    "        s = f.read()\n",
    "        # get the text for parsing\n",
    "        soup = BeautifulSoup(s,\"lxml\")\n",
    "        # select the chunk of html corresponding to each captain listing\n",
    "        content_divs = soup.find('div', attrs={'class':'panel-pane pane-page-content'})\n",
    "        # the type cannot be NoneType\n",
    "        if type(content_divs) != 'NoneType':\n",
    "            # all the captains are inside table tags with cellpadding attribute = 1\n",
    "            picture_tables = content_divs.find_all('table', attrs={'cellpadding':'1'})  #Find *all*\n",
    "            # go through all the tables \n",
    "            for pic in picture_tables:\n",
    "                #select the part where their might be a captain    \n",
    "                pic_captains = pic.select('tr > td > table > tr')\n",
    "                # a captain is present only if length of oic_captains > 1\n",
    "                if(len(pic_captains) > 1):\n",
    "                    # loop through pic_captains from 1 skipping every second one as this is where the captians are \n",
    "                    for j in range(1, len(pic_captains), 2):\n",
    "                        # extract the captain from the html part at the index of pic_captians\n",
    "                        captain = pic_captains[j].select('td > div')\n",
    "                        # only append captain[0].text if the length of captian > 0 (otherwise no captain is present)\n",
    "                        if len(captain) > 0:\n",
    "                            captains.append(captain[0].text)        \n",
    "    return captains\n",
    "captains = get_captains_from_album_urls(au) # gets all the captians\n",
    "print('break')\n",
    "#print(captains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nCharlotte\\n                    Frieze, Eric and Patti Fast, Gregory Long, Julie Graham,\\n                Robin Graham, Fernanda Kellogg, Dominique Browning, and Joan\\n                Khoury\\n']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the objects for use later:\n",
    "with open('objs.pickle', 'wb') as f:\n",
    "    pickle.dump(captains, f) # \n",
    "\n",
    "# Getting back the objects:\n",
    "#with open('objs.pickle') as f:  # Python 3: open(..., 'rb')\n",
    "#    obj0, obj1, obj2 = pickle.load(f)\n",
    "\n",
    "print(len(captains))\n",
    "captains[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import regex\n",
    "\n",
    "#with open('objs.pickle', 'rb') as f:\n",
    "#    captainl = pickle.load(f) \n",
    "\n",
    "#rc = random.choices(range(0, len(captainl)), k = 10)\n",
    "\n",
    "#for r in rc:\n",
    "#    print(captainl[r])\n",
    "\n",
    "def parse_captian(x):\n",
    "    \n",
    "    # tries in a best effort way (not ideal) no extract all the names by themselves form the captains. Uses a list of \n",
    "    # ignored phrases together with a list of abbreviatians for better preson name extraction\n",
    "    \n",
    "    # if the length of the captain is more than 249 then most probably it does not contain names so return an empty list\n",
    "    if(len(x) > 249):\n",
    "        return []\n",
    "    \n",
    "    ignoredPhrases = [\n",
    "        \n",
    "        'Charles Chang-Lima',\n",
    "        'Under the VIP tent',\n",
    "        'Brownies',\n",
    "        'Girl', \n",
    "        '-Guest',\n",
    "        '-Pittel',\n",
    "        'friends',\n",
    "        'The Jewish Museum',\n",
    "        'Knight Landesmen in red (Art Forum)',\n",
    "        'MD',\n",
    "        'FACS',\n",
    "        'PHD',\n",
    "        'The scene at Mary Boone Gallery',\n",
    "        'members',\n",
    "        'member',\n",
    "        'Board Members',\n",
    "        'Board Member and Special Events Committee Chairman',\n",
    "        'Ladies in fuchsia, pink, and red at Bunky Cushing\\'s Valentine\\'s Tea',\n",
    "        'First Lady',\n",
    "        'Governor of Massachusetts',\n",
    "        '2013 Jacob\\'s Pillow Dance Award Winner',\n",
    "        'The Hon.',\n",
    "        'Life Trustee',\n",
    "        'receiving the Mayer Sulzberger Award from',\n",
    "        'from Singapore',\n",
    "        'Chairman Emeritus',\n",
    "        'Ambassador',\n",
    "        'to Jamaica Brenda Johnson',\n",
    "        'Recent Pratt alumni',\n",
    "        'UJA-Federation\\'s 24th Annual Summerfest',\n",
    "        'family',\n",
    "        'Breguet Watches Group',\n",
    "        'NYCB General Manager',\n",
    "        'NYCB Board Chairman',\n",
    "        'his wife',\n",
    "        'East Hampton Library Board Chairman',\n",
    "        'Seated:',\n",
    "        'Standing',\n",
    "        '*',\n",
    "        'member of MSM\\'s International Advisory Board',\n",
    "        'The cake',\n",
    "        'presents the Distinguished Alumni Award to Joseph Macnow \\'67',\n",
    "        'whose Home \"Villa Mille Fiori\" is in the book. Her Family called it \"The Villa\".',\n",
    "        'MAD board chair',\n",
    "        'guests',\n",
    "        'Guitar Maker',\n",
    "        'The tables set for dinner',\n",
    "        'Director',\n",
    "        'Executive',\n",
    "        'in a party:',\n",
    "        'actor',\n",
    "        'in background at Making Musical Waves',\n",
    "        'A dinner for 10 prepared by Chef', \n",
    "        'of the Altamarea Group featuring one pound of Sabatino white truffles auctioned for $50',\n",
    "        'of Ron Ben-Israel Cakes',\n",
    "        'Mrs.',\n",
    "        'Miss Universe',\n",
    "        'Paul Taylor Dance Company',\n",
    "        'Table by',\n",
    "        'ADC',\n",
    "        'Intern',\n",
    "        'Steering Committee',\n",
    "        '\"A Touch of Nature\"',\n",
    "        'of Liberty News',\n",
    "        'of The Beastie Boys',\n",
    "        'of Chicago',\n",
    "        'of Town',\n",
    "        'The Honorable',\n",
    "        'The Rev.',\n",
    "        'their daughter',\n",
    "        'her husband',\n",
    "        'her mother\\'s late husbands.',\n",
    "        'her mother',\n",
    "        'Gala Chair',\n",
    "        'Gala Chairwoman',\n",
    "        'Gala Co Chairs',\n",
    "        'Gala Co-Chair',\n",
    "        'gala concert host',\n",
    "        'Gala Honoree',\n",
    "        'Gala Lead Chairman',\n",
    "        'Gala Vice Chairs',\n",
    "        'Gala Vice Chair',\n",
    "        'Generosity Division Chair',\n",
    "        'Generosity vice chair',\n",
    "        'Museum President',\n",
    "        'OSL Board Chairman',\n",
    "        'OSL Board Member',\n",
    "        'ACO Trustee',\n",
    "        'Board Member',\n",
    "        'Guest of Honor',\n",
    "        '  Prince',\n",
    "        'AMC Trustee',\n",
    "        'Artist',\n",
    "        'Carnegie Hall Trustee',\n",
    "        'Benefit chairman',\n",
    "        'Benefit chair',\n",
    "        'AMC Board',\n",
    "        'Board President',\n",
    "        'NYCB dancers',\n",
    "        'NYCB dancer',\n",
    "        'Honorary',\n",
    "        'Honoree',\n",
    "        'Event Co-Chair-',\n",
    "        'Event Co-Chairs',\n",
    "        'Event Co-Chair',\n",
    "        'Event Co-Chairs:',\n",
    "        'Event Chair',\n",
    "        'Event Chairman',\n",
    "        'Event Chairmen',\n",
    "        'Event Chairwoman',\n",
    "        'Event Chairwomen',\n",
    "        'Frick Trustee'\n",
    "    ]\n",
    "    \n",
    "    abbreviated = [\n",
    "        \n",
    "        ['Kat Von D','Katherine von Drachenberg'],\n",
    "        ['ra Lebenthal','Alexandra Lebenthal'],\n",
    "        ['L.A. Reid','Antonio Marquis'],\n",
    "        ['Jon M. Huntsman','Jon Meade Huntsman'],\n",
    "        ['Kimberly Guilfoyle Villency','Kimberly Guilfoyle'],\n",
    "        ['Ed Sheeran','Edward Christopher'],\n",
    "        ['AlexandAlexandra','Alexandra']\n",
    "    ]\n",
    "    \n",
    "    # replaces occurances of \\xa0\\s with a space\n",
    "    x = regex.sub(r'[\\xa0\\s]+',' ',x)\n",
    "    \n",
    "    # removes ignores phrases\n",
    "    for word in ignoredPhrases:\n",
    "        # \"word + ' '\" is for deleting the trailing whitespace after each  captain.\n",
    "        x = x.replace(word + ' ', '', re.IGNORECASE)\n",
    "    \n",
    "    # replaces the abbreviated names with thier full names\n",
    "    for names in abbreviated:\n",
    "        x = x.replace(names[0], names[1])\n",
    "    \n",
    "    #replaces occurances of spaces followed by a capital letter followed by r and then a point or occurances of brakets: ()\n",
    "    # with whats inside the brakets with a space\n",
    "    x = regex.sub(r'[ ]*[A-Z][r]?[.] *| [(].+[)]',' ',x)\n",
    "    \n",
    "    x = x.strip() #r trims the string removing spaces at the beginning and end\n",
    "    \n",
    "    #splits the string on into smaller strings by removing the seperators: 'K ', ', and ', ', ', ' and ', ' width ', ' & '\n",
    "    # note that a space might not be present after a comma and or with.\n",
    "    rs = regex.split(r'K |(?:, and |, ?| and | with ?| & )?', x)\n",
    "    rs = list(filter(None, rs)) # remove all empty lists from the result \n",
    "    ps = []\n",
    "    \n",
    "    # loop on all names in rs and after parsing  append only the strings which seem to be names \n",
    "    # and disregard others which are not names\n",
    "    for ns in rs:\n",
    "        if ns.startswith('Princess') or ns.startswith('Prince'):\n",
    "            ns =ns.replace(ns[0:ns.find(' ')+1], '')\n",
    "            f = ns.find('of')\n",
    "            if(f != -1):\n",
    "                ns = ns[0:f-1]\n",
    "        \n",
    "        if ns.count(' ') < 6 and not ns[0:12] == 'President of' and not ns[0:6] == 'Guests' and not ns[0:2] == 'A ' \\\n",
    "        and not ns[0:2] == 'a ' and not ns == 'MD' and not ns == 'PhD' and not ns[0:6] == 'CEO of'\\\n",
    "        and not ns == 'friend' and not ns == 'friends' and not ns.strip() == '' and not ns == 'CEO' and not ns == 'his wife'\\\n",
    "        and not ns == 'President':\n",
    "            ps.append(ns)\n",
    "    \n",
    "    return ps\n",
    "\n",
    "captain_names_lists = []\n",
    "# loop an all the captains and exract the names\n",
    "for r in captainl:\n",
    "     captain_names_lists.append(parse_captian(r)) # appends the list of extracted names to the captain_names_lists\n",
    "\n",
    "# store the list of listd inside a csv file for later use\n",
    "with open(\"captain_names.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(captain_names_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       0    1\n",
      "0         Jean Shafiroff  296\n",
      "1        Mark Gilbertson  293\n",
      "2                   John  248\n",
      "3        Gillian Miniter  187\n",
      "4                  David  178\n",
      "5    Alexandra Lebenthal  168\n",
      "6     Geoffrey Bradfield  161\n",
      "7        Kamie Lightburn  160\n",
      "8        Debbie Bancroft  153\n",
      "9          Somers Farkas  150\n",
      "10         Andrew Saffir  149\n",
      "11             Alina Cho  142\n",
      "12    Lucia Hwong Gordon  141\n",
      "13               Michael  140\n",
      "14         Barbara Tober  137\n",
      "15          Mario Buatta  129\n",
      "16                 Peter  126\n",
      "17         Yaz Hernandez  123\n",
      "18           Sharon Bush  122\n",
      "19        Martha Stewart  121\n",
      "20      Eleanora Kennedy  120\n",
      "21      Patrick McMullan  120\n",
      "22               Barbara  120\n",
      "23         Jamee Gregory  115\n",
      "24         Allison Aston  113\n",
      "25        Bettina Zilkha  112\n",
      "26           Lydia Fenet  111\n",
      "27               Richard  110\n",
      "28                Robert  108\n",
      "29   Muffie Potter Aston  108\n",
      "..                   ...  ...\n",
      "70        Bunny Williams   78\n",
      "71              Fe Fendi   77\n",
      "72          Alec Baldwin   77\n",
      "73            Susan Shin   77\n",
      "74             Elizabeth   77\n",
      "75         Susan Solomon   77\n",
      "76                 Nancy   76\n",
      "77  Cassandra Seidenfeld   76\n",
      "78       Nathalie Kaplan   76\n",
      "79     Liliana Cavendish   74\n",
      "80   Dawne Marie Grannum   74\n",
      "81           Denise Rich   74\n",
      "82      Kelly Rutherford   73\n",
      "83        Steven Stolman   73\n",
      "84                   Bob   73\n",
      "85   Elizabeth Stribling   73\n",
      "86       Suzanne Cochran   73\n",
      "87          Pamela Fiori   73\n",
      "88        Charlotte Moss   73\n",
      "89         Evelyn Lauder   73\n",
      "90             Couri Hay   72\n",
      "91         Barbara Regna   72\n",
      "92            Julia Koch   72\n",
      "93       Melanie Holland   71\n",
      "94                  Anne   71\n",
      "95       Richard Mishaan   70\n",
      "96        Felicia Taylor   70\n",
      "97          Gregory Long   70\n",
      "98          Clare McKeon   70\n",
      "99           Donna Karan   69\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.350925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>77.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>88.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                1\n",
       "count  100.000000\n",
       "mean   103.000000\n",
       "std     41.350925\n",
       "min     69.000000\n",
       "25%     77.000000\n",
       "50%     88.500000\n",
       "75%    112.250000\n",
       "max    296.000000"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Names_Edges = []\n",
    "\n",
    "# get all the combinations of two friends form all the lists of names in the captain_names_list and store them in Names_Edges\n",
    "for c in captain_names_lists:\n",
    "    for i in range(0, len(c)):\n",
    "        for j in range(i+1, len(c)):\n",
    "            Names_Edges.append((c[i], c[j])) \n",
    "      \n",
    "g = nx.Graph() # new graph  \n",
    "\n",
    "# if an edge is present between two people add one to the weight. else add a new edge between the two with weight one\n",
    "for e1,e2 in Names_Edges :\n",
    "    if g.has_edge(e1,e2):\n",
    "        g[e1][e2]['weight'] += 1\n",
    "    else :\n",
    "        g.add_edge(e1,e2,weight=1)\n",
    "\n",
    "degrees = g.degree() # the dgree of the graph\n",
    "\n",
    "# sort the degree of the graph\n",
    "ges = sorted(degrees.items(), key=lambda x:x[1], reverse = True)\n",
    "top100e = []\n",
    "\n",
    "# get the top 100 persons with the most degrees (most popular)\n",
    "for i in range(100):\n",
    "    top100e.append([ges[i][0], ges[i][1]])\n",
    "    \n",
    "# print the result and the stats of the results\n",
    "gesf = pd.DataFrame(top100e)\n",
    "print(gesf)\n",
    "gesf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0         1\n",
      "0          Jean Shafiroff  0.000659\n",
      "1                    John  0.000598\n",
      "2         Mark Gilbertson  0.000592\n",
      "3                   David  0.000467\n",
      "4         Gillian Miniter  0.000462\n",
      "5      Geoffrey Bradfield  0.000402\n",
      "6     Alexandra Lebenthal  0.000386\n",
      "7           Andrew Saffir  0.000384\n",
      "8                 Michael  0.000359\n",
      "9         Debbie Bancroft  0.000346\n",
      "10        Kamie Lightburn  0.000335\n",
      "11          Somers Farkas  0.000333\n",
      "12                  Peter  0.000325\n",
      "13                Barbara  0.000325\n",
      "14          Barbara Tober  0.000309\n",
      "15              Alina Cho  0.000289\n",
      "16            Sharon Bush  0.000283\n",
      "17                Richard  0.000281\n",
      "18          Yaz Hernandez  0.000277\n",
      "19                  Susan  0.000275\n",
      "20     Lucia Hwong Gordon  0.000268\n",
      "21       Eleanora Kennedy  0.000267\n",
      "22           Mario Buatta  0.000265\n",
      "23          Jamee Gregory  0.000263\n",
      "24                 Robert  0.000256\n",
      "25         Martha Stewart  0.000244\n",
      "26            Lydia Fenet  0.000241\n",
      "27                   Lisa  0.000240\n",
      "28       Patrick McMullan  0.000233\n",
      "29                 Andrew  0.000228\n",
      "..                    ...       ...\n",
      "70       Margo Langenberg  0.000174\n",
      "71         Steven Stolman  0.000173\n",
      "72   Michele Gerber Klein  0.000173\n",
      "73             CeCe Black  0.000173\n",
      "74            Roric Tobin  0.000171\n",
      "75     Georgina Schaeffer  0.000170\n",
      "76              Elizabeth  0.000170\n",
      "77             Julia Koch  0.000170\n",
      "78            Anka Palitz  0.000169\n",
      "79               Fe Fendi  0.000169\n",
      "80                    Bob  0.000169\n",
      "81  Adelina Wong Ettelson  0.000166\n",
      "82           Clare McKeon  0.000166\n",
      "83          Susan Solomon  0.000165\n",
      "84       Kelly Rutherford  0.000165\n",
      "85              Couri Hay  0.000164\n",
      "86                  Carol  0.000161\n",
      "87        Richard Johnson  0.000160\n",
      "88             Susan Shin  0.000159\n",
      "89                   Anne  0.000158\n",
      "90   Cassandra Seidenfeld  0.000158\n",
      "91                   Bill  0.000157\n",
      "92                  Chris  0.000157\n",
      "93       Alexia Hamm Ryan  0.000157\n",
      "94                   Mark  0.000154\n",
      "95          Barbara Regna  0.000154\n",
      "96              Alexandra  0.000153\n",
      "97            Donna Karan  0.000152\n",
      "98            Denise Rich  0.000152\n",
      "99          Jennifer Raab  0.000152\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                1\n",
       "count  100.000000\n",
       "mean     0.000229\n",
       "std      0.000097\n",
       "min      0.000152\n",
       "25%      0.000171\n",
       "50%      0.000193\n",
       "75%      0.000247\n",
       "max      0.000659"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pageranks = nx.pagerank(g) # page rank of the graph\n",
    "\n",
    "# sort the page rank of the graph\n",
    "ges = sorted(pageranks.items(), key=lambda x:x[1], reverse = True)\n",
    "top100e = []\n",
    "\n",
    "# get the top 100 persons with the higest page rank\n",
    "for i in range(100):\n",
    "    top100e.append([ges[i][0], ges[i][1]])\n",
    "\n",
    "# print the result and the stats of the results\n",
    "gesf = pd.DataFrame(top100e)\n",
    "print(gesf)\n",
    "gesf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          0   1\n",
      "0             (Stewart Lane, Bonnie Comley)  61\n",
      "1          (Andrew Saffir, Daniel Benedict)  53\n",
      "2         (Geoffrey Bradfield, Roric Tobin)  48\n",
      "3        (Jay Diamond, Alexandra Lebenthal)  44\n",
      "4              (Gillian, Sylvester Miniter)  36\n",
      "5                    (Jamee, Peter Gregory)  35\n",
      "6          (Deborah Norville, Karl Wellner)  29\n",
      "7              (Gillian Miniter, Sylvester)  29\n",
      "8       (Guy Robinson, Elizabeth Stribling)  28\n",
      "9   (Sessa von Richthofen, Richard Johnson)  28\n",
      "10         (Hilary Geary Ross, Wilbur Ross)  27\n",
      "11               (Couri Hay, Janna Bullock)  24\n",
      "12                (Marc Rosen, Arlene Dahl)  24\n",
      "13         (Olivia Palermo, Johannes Huebl)  24\n",
      "14                  (Donald Tober, Barbara)  23\n",
      "15   (Jay McInerney, Anne Hearst McInerney)  23\n",
      "16            (Mark Badgley, James Mischka)  23\n",
      "17        (Fernanda Kellogg, Kirk Henckels)  22\n",
      "18    (Frederick Anderson, Douglas Hannant)  21\n",
      "19                   (Peter, Jamee Gregory)  20\n",
      "20            (Tommy Hilfiger, Dee Ocleppo)  20\n",
      "21              (Al Roker, Deborah Roberts)  19\n",
      "22           (Eleanora Kennedy, Anna Safir)  19\n",
      "23                 (Martin Shafiroff, Jean)  19\n",
      "24                 (Chappy, Melissa Morris)  19\n",
      "25            (Jean Shafiroff, Sharon Bush)  18\n",
      "26                   (Peter Regna, Barbara)  18\n",
      "27     (Gillian Miniter, Sylvester Miniter)  18\n",
      "28             (Kim Taipale, Nicole Miller)  17\n",
      "29           (Nina Griscom, Leonel Piraino)  17\n",
      "..                                      ...  ..\n",
      "70  (Diana Taylor, Mayor Michael Bloomberg)  12\n",
      "71           (Marion Waxman, Samuel Waxman)  12\n",
      "72        (Johannes Huebl, Daniel Benedict)  12\n",
      "73             (Phil Donahue, Marlo Thomas)  12\n",
      "74             (Charlotte Ronson, Ali Wise)  12\n",
      "75           (Nancy Wexler, Herbert Pardes)  12\n",
      "76               (Will Cotton, Rose Dergan)  12\n",
      "77     (Caroline Murphy, Heather Matarazzo)  12\n",
      "78                   (Grace, Chris Meigher)  11\n",
      "79     (Jean Shafiroff, Lucia Hwong Gordon)  11\n",
      "80           (Peter Martins, Darci Kistler)  11\n",
      "81              (Michael, Eleanora Kennedy)  11\n",
      "82           (Harry Slatkin, Laura Slatkin)  11\n",
      "83           (Polina Proshkina, Yan Assoun)  11\n",
      "84           (Tatiana Platt, Campion Platt)  11\n",
      "85          (Edwina Sandys, Richard Kaplan)  11\n",
      "86                    (Coco, Arie Kopelman)  11\n",
      "87      (Yaz Hernandez, Valentin Hernandez)  11\n",
      "88           (Simon Doonan, Jonathan Adler)  11\n",
      "89         (Sharon Sondes, Geoffrey Thomas)  11\n",
      "90          (Alex McCord, Simon van Kempen)  11\n",
      "91           (Lorenzo Martone, Marc Jacobs)  11\n",
      "92               (Diane Passage, Ken Starr)  11\n",
      "93        (Bettina Zilkha, Debbie Bancroft)  10\n",
      "94                  (Barbara Tober, Donald)  10\n",
      "95               (Fe Fendi, Patricia Shiah)  10\n",
      "96                      (David Koch, Julia)  10\n",
      "97          (Gillian Miniter, Alexis Clark)  10\n",
      "98   (Gillian Miniter, Alexandra Lebenthal)  10\n",
      "99                (Jay Johnson, Tom Cashin)  10\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.063948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                1\n",
       "count  100.000000\n",
       "mean    16.920000\n",
       "std      9.063948\n",
       "min     10.000000\n",
       "25%     12.000000\n",
       "50%     14.000000\n",
       "75%     18.250000\n",
       "max     61.000000"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the edges by the weight of each edge\n",
    "ges = sorted(g.edges(data=True), key=lambda data: data[2]['weight'], reverse = True)\n",
    "top100e = []\n",
    "# get the highest 100 edges (top 100 Best Friends)\n",
    "for i in range(100):\n",
    "    top100e.append(((ges[i][0], ges[i][1]), ges[i][2]['weight']))\n",
    "\n",
    "# print the result and the stats of the results\n",
    "gesf = pd.DataFrame(top100e)\n",
    "print(gesf)\n",
    "gesf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
